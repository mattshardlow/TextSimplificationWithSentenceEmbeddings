{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSBk2BJxG6VL"
      },
      "outputs": [],
      "source": [
        "#install SONAR - will be prompted to restart environment (wait until cell execution is complete)\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install fairseq2==0.3.0rc1 --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cu124\n",
        "!pip install sonar-space==0.3.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/feralvam/easse.git"
      ],
      "metadata": {
        "id": "V0jPtm_RG-oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wiki Auto - simplification\n",
        "! wget https://raw.githubusercontent.com/chaojiang06/wiki-auto/refs/heads/master/wiki-auto/ACL2020/train.dst\n",
        "! wget https://raw.githubusercontent.com/chaojiang06/wiki-auto/refs/heads/master/wiki-auto/ACL2020/train.src"
      ],
      "metadata": {
        "id": "G0yCcq7YHAV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "bq_H8lMxDtCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all source and target sentences go in here (dict of dicts: dataset-name : src [], tgt [])\n",
        "all_sentences = {}"
      ],
      "metadata": {
        "id": "pnKrQhbAHDxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Asset\n",
        "asset_path = \"/content/easse/easse/resources/data/test_sets/asset/\"\n",
        "\n",
        "asset_original_val_path = asset_path + \"asset.valid.orig\"\n",
        "asset_original_val_sentences = open(asset_original_val_path, \"r\").readlines()\n",
        "\n",
        "for i in range(10):\n",
        "  name = \"asset.valid.simp.\" + str(i)\n",
        "  asset_simp_val_path = asset_path + name\n",
        "  asset_simp_val_sentences = open(asset_simp_val_path, \"r\").readlines()\n",
        "  all_sentences[name] = {\"src\": asset_original_val_sentences, \"tgt\": asset_simp_val_sentences}\n",
        "\n",
        "asset_original_test_path = asset_path + \"asset.test.orig\"\n",
        "asset_original_test_sentences = open(asset_original_test_path, \"r\").readlines()\n",
        "\n",
        "for i in range(10):\n",
        "  name = \"asset.test.simp.\" + str(i)\n",
        "  asset_simp_test_path = asset_path + \"asset.test.simp.\" + str(i)\n",
        "  asset_simp_test_sentences = open(asset_simp_test_path, \"r\").readlines()\n",
        "  all_sentences[name] = {\"src\": asset_original_test_sentences, \"tgt\": asset_simp_test_sentences}\n",
        "\n"
      ],
      "metadata": {
        "id": "whlTwGz4HEMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wiki auto import\n",
        "wiki_auto_complex = open(\"/content/train.src\", \"r\").readlines()\n",
        "wiki_auto_simple = open(\"/content/train.dst\", \"r\").readlines()\n",
        "\n",
        "all_sentences['wiki_auto'] = {\"src\": wiki_auto_complex, \"tgt\": wiki_auto_simple}"
      ],
      "metadata": {
        "id": "WQT4Wj_kHGHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE = torch.device(DEVICE)\n",
        "torch.set_grad_enabled(False)\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "id": "9Lf2hbO3D2Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set up SONAR models - TextToEmbeddingModelPipeline for encoding and EmbeddingToTextModelPipeline for decoding\n",
        "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
        "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
        "\n",
        "\n",
        "# load models\n",
        "text2vec = TextToEmbeddingModelPipeline(encoder=\"text_sonar_basic_encoder\", tokenizer=\"text_sonar_basic_encoder\", device=DEVICE)\n",
        "vec2text = EmbeddingToTextModelPipeline(decoder=\"text_sonar_basic_decoder\", tokenizer=\"text_sonar_basic_encoder\", device=DEVICE)"
      ],
      "metadata": {
        "id": "YczuS8F1HH2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode sentences with SONAR (f(x))\n",
        "b_size = 64\n",
        "\n",
        "embeddings = {}\n",
        "\n",
        "val_count = 2000\n",
        "\n",
        "#val\n",
        "embeddings['wauto_comp_val'] = text2vec.predict(all_sentences['wiki_auto']['src'][:val_count],   source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "embeddings['wauto_simp_val'] = text2vec.predict(all_sentences['wiki_auto']['tgt'][:val_count],   source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "\n",
        "#train\n",
        "embeddings['asset_comp_train'] = text2vec.predict(all_sentences['asset.valid.simp.0']['src'], source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "embeddings['asset_simp_train'] = text2vec.predict(all_sentences['asset.valid.simp.0']['tgt'], source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "\n",
        "embeddings['wauto_comp_train'] = text2vec.predict(all_sentences['wiki_auto']['src'][val_count:],   source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "embeddings['wauto_simp_train'] = text2vec.predict(all_sentences['wiki_auto']['tgt'][val_count:],   source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "\n",
        "#test\n",
        "embeddings['asset_comp_test']  = text2vec.predict(all_sentences['asset.test.simp.0']['src'],  source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "embeddings['asset_simp_test']  = text2vec.predict(all_sentences['asset.test.simp.0']['tgt'],  source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)"
      ],
      "metadata": {
        "id": "ORVHWNpYHsf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it may be useful to save the embeddings to memory, restart the environment to clear the GPU RAM and then reload from the cell below.\n",
        "\n",
        "#pickle.dump(embeddings, open('embeddings.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "LDNqZLxBVUmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embeddings = pickle.load(open('embeddings.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "HoIvYU3gVbiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_train = torch.cat((embeddings['asset_comp_train'],embeddings['wauto_comp_train']), dim=0)\n",
        "tgt_train = torch.cat((embeddings['asset_simp_train'],embeddings['wauto_simp_train']), dim=0)"
      ],
      "metadata": {
        "id": "PaWQD67NH_RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up NN - g(x)\n",
        "#basic feed-forward neural network with ADAM optimiser\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleFeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(SimpleFeedForward, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the models\n",
        "input_dim = src_train.size()[1]\n",
        "output_dim = tgt_train.size()[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "280LtduyHx0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_val = embeddings['wauto_comp_val']\n",
        "tgt_val = embeddings['wauto_simp_val']"
      ],
      "metadata": {
        "id": "8kMd0bhwIx8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# again - if needed, you can dump the training data to disk here and then reload in a clean GPU from the cell below\n",
        "\n",
        "#pickle.dump(src_train, open('src_train.pkl', 'wb'))\n",
        "#pickle.dump(tgt_train, open('tgt_train.pkl', 'wb'))\n",
        "#pickle.dump(src_val, open('src_val.pkl', 'wb'))\n",
        "#pickle.dump(tgt_val, open('tgt_val.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "AbbA6SenIz1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#src_train = pickle.load(open('src_train.pkl', 'rb'))\n",
        "#tgt_train = pickle.load(open('tgt_train.pkl', 'rb'))\n",
        "#src_val = pickle.load(open('src_val.pkl', 'rb'))\n",
        "#tgt_val = pickle.load(open('tgt_val.pkl', 'rb'))"
      ],
      "metadata": {
        "id": "OkSLvUxyI7X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run the training loop (AI generated initial copy - caveat emptor)\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.MSELoss()  # Mean Squared Error\n",
        "\n",
        "# Move data to the same device as the model (not needed if you reloaded from disk)\n",
        "#src_train = src_train.clone()\n",
        "#tgt_train = tgt_train.clone()\n",
        "\n",
        "\n",
        "# Enable gradient calculation for training\n",
        "torch.set_grad_enabled(True)\n",
        "\n",
        "def train_loop(id, model, src, tgt, val_src, val_tgt, lr=0.001, epochs=5000):\n",
        "  log_file = open(\"%s.log\" % id, \"w\")\n",
        "\n",
        "  model.train()\n",
        "  print(id)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  initial_train_loss = criterion(model(src),tgt).item()\n",
        "  print(\"initial train loss:\", initial_train_loss)\n",
        "  initial_val_loss = criterion(model(val_src),val_tgt).item()\n",
        "  print(\"initial val loss:\", initial_val_loss)\n",
        "\n",
        "  log_file.write(\"%d,%.10f,%.10f\\n\"%(0,initial_train_loss,initial_val_loss))\n",
        "\n",
        "  best_val_loss = initial_val_loss\n",
        "  best_epoch = 0\n",
        "  loss = None\n",
        "\n",
        "  # Training loop\n",
        "  num_epochs = epochs\n",
        "  for epoch in range(num_epochs):\n",
        "    outputs = model(src)\n",
        "    loss = criterion(outputs, tgt)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        model.eval()\n",
        "        val_outputs = model(val_src)\n",
        "        val_loss = criterion(val_outputs, val_tgt)\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Val Loss: {val_loss.item()}',end='')\n",
        "        log_file.write(\"%d,%.10f,%.10f\\n\"%(epoch+1,loss.item(),val_loss.item()))\n",
        "        if val_loss.item() < best_val_loss:\n",
        "          print(\"*\",end='')\n",
        "          best_val_loss = val_loss.item()\n",
        "          best_epoch = epoch\n",
        "          torch.save(model, \"best_model%s.pt\" % id)\n",
        "        else:\n",
        "          if epoch - best_epoch > 250:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "        model.train()\n",
        "        print()\n",
        "\n",
        "  log_file.close()\n",
        "  print(\"Training finished.\")\n",
        "\n",
        "\n",
        "#run training for each model\n",
        "for k in [4096]:# [256,512,1024,2048,4096]:\n",
        "  model = SimpleFeedForward(input_dim, k, output_dim).to(DEVICE)\n",
        "  print(model)\n",
        "  pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(pytorch_total_params)\n",
        "  train_loop(\"ASSET - %d\"%k, model, src_train, tgt_train, src_val, tgt_val, epochs=10000)\n",
        "\n",
        "# Disable gradient calculation after training if needed for inference\n",
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "id": "rTHEhYoIH2HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViH_U3AhEGVq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}