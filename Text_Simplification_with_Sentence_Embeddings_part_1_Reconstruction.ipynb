{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ0V_2bXIcf-"
      },
      "outputs": [],
      "source": [
        "#install SONAR - will be prompted to restart environment (wait until cell execution is complete)\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install fairseq2==0.3.0rc1 --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/rc/pt2.5.1/cu124\n",
        "!pip install sonar-space==0.3.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/feralvam/easse.git"
      ],
      "metadata": {
        "id": "b2X4y-pmKjbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wiki Auto - simplification\n",
        "! wget https://raw.githubusercontent.com/chaojiang06/wiki-auto/refs/heads/master/wiki-auto/ACL2020/train.dst\n",
        "! wget https://raw.githubusercontent.com/chaojiang06/wiki-auto/refs/heads/master/wiki-auto/ACL2020/train.src"
      ],
      "metadata": {
        "id": "nk0ee0DOKuz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all source and target sentences go in here (dict of dicts: dataset-name : src [], tgt [])\n",
        "all_sentences = {}"
      ],
      "metadata": {
        "id": "t3R_abiHKx6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Asset\n",
        "asset_path = \"/content/easse/easse/resources/data/test_sets/asset/\"\n",
        "\n",
        "asset_original_val_path = asset_path + \"asset.valid.orig\"\n",
        "asset_original_val_sentences = open(asset_original_val_path, \"r\").readlines()\n",
        "\n",
        "for i in range(10):\n",
        "  name = \"asset.valid.simp.\" + str(i)\n",
        "  asset_simp_val_path = asset_path + name\n",
        "  asset_simp_val_sentences = open(asset_simp_val_path, \"r\").readlines()\n",
        "  all_sentences[name] = {\"src\": asset_original_val_sentences, \"tgt\": asset_simp_val_sentences}\n",
        "\n",
        "asset_original_test_path = asset_path + \"asset.test.orig\"\n",
        "asset_original_test_sentences = open(asset_original_test_path, \"r\").readlines()\n",
        "\n",
        "for i in range(10):\n",
        "  name = \"asset.test.simp.\" + str(i)\n",
        "  asset_simp_test_path = asset_path + \"asset.test.simp.\" + str(i)\n",
        "  asset_simp_test_sentences = open(asset_simp_test_path, \"r\").readlines()\n",
        "  all_sentences[name] = {\"src\": asset_original_test_sentences, \"tgt\": asset_simp_test_sentences}\n",
        "\n"
      ],
      "metadata": {
        "id": "3YMYx9XRLeWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Wiki auto import\n",
        "wiki_auto_complex = open(\"/content/train.src\", \"r\").readlines()\n",
        "wiki_auto_simple = open(\"/content/train.dst\", \"r\").readlines()\n",
        "\n",
        "all_sentences['wiki_auto'] = {\"src\": wiki_auto_complex, \"tgt\": wiki_auto_simple}"
      ],
      "metadata": {
        "id": "7JvU_KDxLgvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set up SONAR models - TextToEmbeddingModelPipeline for encoding and EmbeddingToTextModelPipeline for decoding\n",
        "import torch\n",
        "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
        "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE = torch.device(DEVICE)\n",
        "torch.set_grad_enabled(False)\n",
        "print(DEVICE)\n",
        "\n",
        "# load models\n",
        "text2vec = TextToEmbeddingModelPipeline(encoder=\"text_sonar_basic_encoder\", tokenizer=\"text_sonar_basic_encoder\", device=DEVICE)\n",
        "vec2text = EmbeddingToTextModelPipeline(decoder=\"text_sonar_basic_decoder\", tokenizer=\"text_sonar_basic_encoder\", device=DEVICE)"
      ],
      "metadata": {
        "id": "SPilMp_QPusV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Experiment 1\n",
        "# - encode sentences with SONAR + reconstruct\n",
        "b_size = 64\n",
        "\n",
        "embeddings = {}\n",
        "embeddings['asset_comp_train'] = text2vec.predict(all_sentences['asset.valid.simp.0']['src'],  source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "embeddings['asset_simp_train']  = text2vec.predict(all_sentences['asset.valid.simp.0']['tgt'], source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "embeddings['wauto_comp_train'] = text2vec.predict(all_sentences['wiki_auto']['src'][:2000], source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "embeddings['wauto_simp_train']  = text2vec.predict(all_sentences['wiki_auto']['tgt'][:2000], source_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size)\n",
        "\n",
        "\n",
        "reconstruction = {}\n",
        "reconstruction['asset_comp_train'] = vec2text.predict(embeddings['asset_comp_train'],  target_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size, len_penalty=0.8)\n",
        "reconstruction['asset_simp_train'] = vec2text.predict(embeddings['asset_simp_train'],  target_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size, len_penalty=0.8)\n",
        "reconstruction['wauto_comp_train'] = vec2text.predict(embeddings['wauto_comp_train'],  target_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size, len_penalty=0.8)\n",
        "reconstruction['wauto_simp_train'] = vec2text.predict(embeddings['wauto_simp_train'],  target_lang=\"eng_Latn\", max_seq_len=128, progress_bar=True, batch_size=b_size, len_penalty=0.8)"
      ],
      "metadata": {
        "id": "nScRQIRYNt_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "NN0A3dJ_H_K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(all_sentences, open(\"all_sentences.pkl\", \"wb\"))\n",
        "pickle.dump(embeddings, open(\"embeddings.pkl\", \"wb\"))\n",
        "pickle.dump(reconstruction, open(\"reconstruction.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "T9a4g81iICv_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}